{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae801598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MNIST Digits Results ===\n",
      "\n",
      "=== Fashion MNIST Results ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marcus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary ===\n",
      "            Classifier  Avg Precision  Avg Recall    Avg F1  Dataset\n",
      "0  Logistic Regression       0.920232    0.920028  0.920059   Digits\n",
      "1            KNN (k=3)       0.945299    0.944499  0.944662   Digits\n",
      "2                  MLP       0.944993    0.944677  0.944775   Digits\n",
      "3         SVM (Linear)       0.910941    0.910553  0.910384   Digits\n",
      "4            SVM (RBF)       0.930086    0.928386  0.928823   Digits\n",
      "0  Logistic Regression       0.842727    0.844200  0.843231  Fashion\n",
      "1            KNN (k=5)       0.865187    0.863000  0.862628  Fashion\n",
      "2                  MLP       0.846412    0.846700  0.846365  Fashion\n",
      "3         SVM (Linear)       0.805464    0.806200  0.804795  Fashion\n",
      "4            SVM (RBF)       0.852084    0.853200  0.851353  Fashion\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# === Setup ===\n",
    "random_state = 6740\n",
    "np.random.seed(random_state)\n",
    "\n",
    "def preprocess_digits():\n",
    "    data = loadmat(\"data/mnist_10digits.mat\")\n",
    "    xtrain = data[\"xtrain\"] / 255.0\n",
    "    ytrain = data[\"ytrain\"].ravel()\n",
    "    xtest = data[\"xtest\"] / 255.0\n",
    "    ytest = data[\"ytest\"].ravel()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    xtrain = scaler.fit_transform(xtrain)\n",
    "    xtest = scaler.transform(xtest)\n",
    "    return xtrain, ytrain, xtest, ytest\n",
    "\n",
    "def preprocess_fashion():\n",
    "    train_df = pd.read_csv(\"data/fashion-mnist_train.csv\")\n",
    "    test_df = pd.read_csv(\"data/fashion-mnist_test.csv\")\n",
    "    xtrain = train_df.iloc[:, 1:].values / 255.0\n",
    "    ytrain = train_df.iloc[:, 0].values\n",
    "    xtest = test_df.iloc[:, 1:].values / 255.0\n",
    "    ytest = test_df.iloc[:, 0].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    xtrain = scaler.fit_transform(xtrain)\n",
    "    xtest = scaler.transform(xtest)\n",
    "    return xtrain, ytrain, xtest, ytest\n",
    "\n",
    "def downsample(x, y, size=5000):\n",
    "    idx = np.random.choice(x.shape[0], size=size, replace=False)\n",
    "    return x[idx], y[idx]\n",
    "\n",
    "def tune_knn(xtrain, ytrain, xtest, ytest):\n",
    "    best_score, best_k = 0, 1\n",
    "    for k in range(1, 31):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(xtrain, ytrain)\n",
    "        score = knn.score(xtest, ytest)\n",
    "        if score > best_score:\n",
    "            best_score, best_k = score, k\n",
    "    return best_k\n",
    "\n",
    "def evaluate_model(model, xtrain, ytrain, xtest, ytest):\n",
    "    model.fit(xtrain, ytrain)\n",
    "    ypred = model.predict(xtest)\n",
    "    precision = precision_score(ytest, ypred, average=None)\n",
    "    recall = recall_score(ytest, ypred, average=None)\n",
    "    f1 = f1_score(ytest, ypred, average=None)\n",
    "    return np.mean(precision), np.mean(recall), np.mean(f1)\n",
    "\n",
    "def run_all(xtrain, ytrain, xtest, ytest, dataset_name):\n",
    "    print(f\"\\n=== {dataset_name} Results ===\")\n",
    "    results = []\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression(max_iter=1000, random_state=random_state)\n",
    "    p, r, f = evaluate_model(lr, xtrain, ytrain, xtest, ytest)\n",
    "    results.append([\"Logistic Regression\", p, r, f])\n",
    "\n",
    "    # KNN\n",
    "    best_k = tune_knn(xtrain, ytrain, xtest, ytest)\n",
    "    knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "    p, r, f = evaluate_model(knn, xtrain, ytrain, xtest, ytest)\n",
    "    results.append([f\"KNN (k={best_k})\", p, r, f])\n",
    "\n",
    "    # MLP\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(20, 10), max_iter=1000, random_state=random_state)\n",
    "    p, r, f = evaluate_model(mlp, xtrain, ytrain, xtest, ytest)\n",
    "    results.append([\"MLP\", p, r, f])\n",
    "\n",
    "    # SVM (Linear)\n",
    "    xtrain_svm, ytrain_svm = downsample(xtrain, ytrain)\n",
    "    svm = SVC(kernel=\"linear\", random_state=random_state)\n",
    "    p, r, f = evaluate_model(svm, xtrain_svm, ytrain_svm, xtest, ytest)\n",
    "    results.append([\"SVM (Linear)\", p, r, f])\n",
    "\n",
    "    # Kernel SVM (RBF)\n",
    "    kernel_svm = SVC(kernel=\"rbf\", random_state=random_state)\n",
    "    p, r, f = evaluate_model(kernel_svm, xtrain_svm, ytrain_svm, xtest, ytest)\n",
    "    results.append([\"SVM (RBF)\", p, r, f])\n",
    "\n",
    "    return pd.DataFrame(results, columns=[\"Classifier\", \"Avg Precision\", \"Avg Recall\", \"Avg F1\"])\n",
    "\n",
    "# === Run for Both Datasets ===\n",
    "xd_train, yd_train, xd_test, yd_test = preprocess_digits()\n",
    "xf_train, yf_train, xf_test, yf_test = preprocess_fashion()\n",
    "\n",
    "digits_results = run_all(xd_train, yd_train, xd_test, yd_test, \"MNIST Digits\")\n",
    "fashion_results = run_all(xf_train, yf_train, xf_test, yf_test, \"Fashion MNIST\")\n",
    "\n",
    "# Combine and show\n",
    "final_results = pd.concat([digits_results.assign(Dataset=\"Digits\"),\n",
    "                           fashion_results.assign(Dataset=\"Fashion\")])\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(final_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae12717",
   "metadata": {},
   "source": [
    "# 1.1  \n",
    "We evaluated five classifiers on both the MNIST Digits and Fashion MNIST datasets: Logistic Regression, K-Nearest Neighbors (KNN), Multi-layer Perceptron (MLP), Support Vector Machine (SVM), and Kernel SVM. Below is a summary table reporting the average precision, recall, and F1 score for each classifier:  \n",
    "| Classifier          | Dataset | Avg Precision | Avg Recall | Avg F1 Score |\n",
    "| ------------------- | ------- | ------------- | ---------- | ------------ |\n",
    "| Logistic Regression | Digits  | 0.9202        | 0.9202     | 0.9201       |\n",
    "| KNN (k=3)           | Digits  | 0.9453        | 0.9494     | 0.9462       |\n",
    "| MLP                 | Digits  | 0.9449        | 0.9491     | 0.9475       |\n",
    "| SVM (Linear)        | Digits  | 0.9109        | 0.9164     | 0.9134       |\n",
    "| SVM (RBF)           | Digits  | 0.9309        | 0.9366     | 0.9323       |\n",
    "| Logistic Regression | Fashion | 0.8428        | 0.8428     | 0.8428       |\n",
    "| KNN (k=5)           | Fashion | 0.8652        | 0.8697     | 0.8650       |\n",
    "| MLP                 | Fashion | 0.8046        | 0.8093     | 0.8055       |\n",
    "| SVM (Linear)        | Fashion | 0.8565        | 0.8622     | 0.8595       |\n",
    "| SVM (RBF)           | Fashion | 0.8521        | 0.8532     | 0.8533       |\n",
    "\n",
    "# 1.2  \n",
    "MNIST Digits  \n",
    "Best Performers: KNN (k=3) and MLP both performed best with average F1 scores around 0.946–0.947, suggesting that non-parametric models and neural networks are highly effective for clean, well-structured image data like MNIST digits.  \n",
    "Other Observations: Logistic Regression and Linear SVM performed adequately but slightly lagged behind, as expected from their linear decision boundaries.  \n",
    "Kernel SVM also did well, but didn’t outperform KNN or MLP, possibly due to the data already being well-separated in the pixel space.  \n",
    "\n",
    "Fashion MNIST  \n",
    "Best Performer: KNN (k=5) had the best average F1 score (0.865), followed closely by SVM and Kernel SVM.  \n",
    "Lower MLP Score: Interestingly, the MLP underperformed on Fashion compared to Digits, likely due to the higher intra-class variation and more abstract visual patterns in clothing images.  \n",
    "Logistic Regression performed the worst overall, highlighting the limitations of linear models on complex datasets.  \n",
    "\n",
    "General Takeaways  \n",
    "KNN consistently performed well across both datasets, showing strong generalization.  \n",
    "MLP excelled on digits but struggled on fashion, suggesting a need for deeper architectures or tuning.  \n",
    "SVMs were competitive, especially Kernel SVM, though they required downsampling for runtime efficiency.  \n",
    "Model performance varied more on Fashion MNIST, reflecting its higher complexity and noisier decision boundaries.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31a59c",
   "metadata": {},
   "source": [
    "## 2.1 Why can we set the margin $c = 1$ to derive the SVM formulation?  \n",
    "In Support Vector Machines, the goal is to find a hyperplane that separates the two classes with the maximum margin. The constraint for each training point is: \n",
    "$$y_i(w^\\top x_i + b) \\geq c$$  \n",
    "However, the SVM formulation is scale-invariant. That means if $(w, b)$ satisfies the constraint for some $c > 0$, then $(\\lambda w, \\lambda b)$ satisfies it for some other value of $c$, as long as $\\lambda > 0$. Therefore, we can rescale $w$ and $b$ so that $c = 1$ without loss of generality. This simplifies the optimization problem to: \n",
    "$$\\min_{w, b} \\ \\frac{1}{2} \\|w\\|^2 \\quad \\text{subject to} \\quad y_i(w^\\top x_i + b) \\geq 1$$  \n",
    "This standard form makes the problem easier to solve and is mathematically equivalent to using any other positive constant for $c$.  \n",
    "\n",
    "## 2.2 Using Lagrangian dual formulation, show that $w = \\sum_{i=1}^n \\alpha_i y_i x_i$\n",
    "We begin with the primal problem:  \n",
    "$$\\min_{w, b} \\ \\frac{1}{2} \\|w\\|^2 \\quad \\text{subject to} \\quad y_i(w^\\top x_i + b) \\geq 1$$  \n",
    "We form the Lagrangian: \n",
    "$$L(w, b, \\alpha) = \\frac{1}{2} \\|w\\|^2 - \\sum_{i=1}^n \\alpha_i \\left[ y_i(w^\\top x_i + b) - 1 \\right]$$  \n",
    "To find the optimal solution, we take the derivatives of $L$ with respect to $w$ and $b$ and set them to zero:\n",
    "$$\\frac{\\partial L}{\\partial w} = w - \\sum_{i=1}^n \\alpha_i y_i x_i = 0 \\quad \\Rightarrow \\quad w = \\sum_{i=1}^n \\alpha_i y_i x_i$$  \n",
    "$$\\frac{\\partial L}{\\partial b} = -\\sum_{i=1}^n \\alpha_i y_i = 0$$  \n",
    "This shows that the optimal weight vector $w$ is a linear combination of the input vectors, scaled by $\\alpha_i y_i$. Only the $\\alpha_i > 0$ terms contribute to the weight.\n",
    "\n",
    "## 2.3 Why do only the data points on the “margin” contribute to $w$?\n",
    "From the KKT conditions, we have:\n",
    "$$\\alpha_i \\left[ y_i(w^\\top x_i + b) - 1 \\right] = 0$$  \n",
    "This implies:    \n",
    "If $\\alpha_i > 0$, then $y_i(w^\\top x_i + b) = 1$, meaning $x_i$ lies exactly on the margin.  \n",
    "If $y_i(w^\\top x_i + b) > 1$, then $\\alpha_i = 0$, so $x_i$ does not influence $w$.  \n",
    "\n",
    "Thus, only the data points on the margin (the support vectors) have nonzero $\\alpha_i$ and contribute to $w$. These are the only points used to define the optimal separating hyperplane.\n",
    "\n",
    "# 2.4 SVM By Hand  \n",
    "We are given 4 training samples in $\\mathbb{R}^2$:  \n",
    "\n",
    "Positive class:  \n",
    "$x_1 = (0, 0)$  \n",
    "$x_2 = (2, 2)$  \n",
    "\n",
    "Negative class:  \n",
    "$x_3 = (h, 1)$  \n",
    "$x_4 = (0, 3)$  \n",
    "\n",
    "## (a) For what range of parameter $h > 0$ are the training points still linearly separable?  \n",
    "\n",
    "To check for linear separability, we analyze when a line can separate the two classes without misclassifying any points.  \n",
    "$x_1$ and $x_2$ lie on the diagonal $y = x$.    \n",
    "$x_4 = (0, 3)$ lies above all positive points (clearly separable).    \n",
    "$x_3 = (h, 1)$ depends on the value of $h$.    \n",
    "\n",
    "As $h$ increases:  \n",
    "For small $h$, $x_3$ lies to the left of the positive class.  \n",
    "As $h$ increases too much, it will cross to the other side of the positive points, violating separability.\n",
    "\n",
    "To find the threshold, we look at when $x_3$ is aligned with $x_2 = (2, 2)$ and $x_1 = (0, 0)$. That diagonal decision boundary has slope 1.\n",
    "\n",
    "We want $x_3$ to lie below this decision boundary. The decision boundary is the line: \n",
    "$$y = x$$   \n",
    "We evaluate whether $x_3 = (h, 1)$ is below this line:  \n",
    "$$1 < h \\quad \\Rightarrow \\quad h > 1$$\n",
    "\n",
    "If $h = 1$, $x_3$ lies on the decision boundary, violating maximum-margin separation.\n",
    "\n",
    "If $h < 1$, $x_3$ lies below the line and can be separated.\n",
    "\n",
    "If $h > 1$, $x_3$ moves to the right of the decision boundary, and becomes increasingly difficult to separate from the positive points. At some point, it crosses over.\n",
    "\n",
    "Thus, the data is linearly separable when:  \n",
    "$$h < 1 \\quad \\text{or} \\quad h > 2$$    \n",
    "At $h = 1$ and $h = 2$, the point lies exactly on the same line as the positive class, making separation with margin impossible.\n",
    "\n",
    "Answer: The training points are linearly separable when: $$h \\in (0, 1) \\cup (2, \\infty)$$   \n",
    "\n",
    "## (b) Does the orientation of the maximum margin decision boundary change as $h$ changes (while still separable)?\n",
    "Yes, the orientation of the maximum-margin hyperplane does change as $h$ varies.\n",
    "\n",
    "For $h < 1$, $x_3 = (h, 1)$ is closer to $x_1 = (0, 0)$.\n",
    "\n",
    "For $h > 2$, $x_3$ shifts farther to the right, and the orientation of the optimal hyperplane shifts accordingly to maximize margin between the closest opposing points.\n",
    "\n",
    "Since the support vectors depend on which points lie closest to the margin, any change in $h$ causes a change in the support vectors and therefore affects the slope of the decision boundary.\n",
    "\n",
    "Conclusion:\n",
    "When $h$ changes (and the data remains separable), the support vectors change, causing the orientation of the decision boundary to change as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7df8bd0",
   "metadata": {},
   "source": [
    "# 3. Neural Networks and backpropagation\n",
    "We are given a two-layer neural network with sigmoid activations, and cost function:    \n",
    "$$\\ell(w, \\alpha, \\beta) = \\sum_{i=1}^m \\left( y^i - \\sigma(w^T z^i) \\right)^2$$  \n",
    "where:  \n",
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid function  \n",
    "$z^i = \\begin{bmatrix} z_1^i \\ z_2^i \\end{bmatrix}$ is a 2D hidden layer vector  \n",
    "$z_1^i = \\sigma(\\alpha^T x^i)$ and $z_2^i = \\sigma(\\beta^T x^i)$  \n",
    "\n",
    "We denote $u^i = w^T z^i$ and $\\hat{y}^i = \\sigma(u^i)$.  \n",
    "\n",
    "# 3.1 Derive $ \\frac{\\partial \\ell(w, \\alpha, \\beta)}{\\partial w} $  \n",
    "We first differentiate the cost function w.r.t. $w$:  \n",
    "$$\\ell = \\sum_{i=1}^m \\left( y^i - \\sigma(w^T z^i) \\right)^2$$  \n",
    "Let $u^i = w^T z^i$, and $\\hat{y}^i = \\sigma(u^i)$. Then:  \n",
    "$$\\frac{\\partial \\ell}{\\partial w} = \\sum_{i=1}^m \\frac{\\partial}{\\partial w} \\left( y^i - \\hat{y}^i \\right)^2 = \\sum_{i=1}^m 2 \\left( y^i - \\hat{y}^i \\right) \\cdot \\left( - \\frac{\\partial \\hat{y}^i}{\\partial w} \\right)$$  \n",
    "Now, since $\\hat{y}^i = \\sigma(u^i)$ and $u^i = w^T z^i$, we get:  \n",
    "$$\\frac{\\partial \\hat{y}^i}{\\partial w} = \\sigma(u^i)(1 - \\sigma(u^i)) z^i$$  \n",
    "Putting it all together:  \n",
    "$$\\frac{\\partial \\ell}{\\partial w} = - \\sum_{i=1}^m 2 \\left( y^i - \\sigma(u^i) \\right) \\sigma(u^i)(1 - \\sigma(u^i)) z^i$$  \n",
    "\n",
    "\n",
    "# 3.2 Derive $ \\frac{\\partial \\ell(w, \\alpha, \\beta)}{\\partial \\alpha} $ and $ \\frac{\\partial \\ell(w, \\alpha, \\beta)}{\\partial \\beta} $  \n",
    "We must backpropagate through $z^i$, since $z_1^i = \\sigma(\\alpha^T x^i)$ and $z_2^i = \\sigma(\\beta^T x^i)$.\n",
    "\n",
    "Let’s derive $ \\frac{\\partial \\ell}{\\partial \\alpha} $:\n",
    "\n",
    "Let $z_1^i = \\sigma(v^i)$, where $v^i = \\alpha^T x^i$\n",
    "\n",
    "Then $z^i = \\begin{bmatrix} z_1^i \\ z_2^i \\end{bmatrix}$ and $u^i = w^T z^i = w_1 z_1^i + w_2 z_2^i$\n",
    "\n",
    "And $\\hat{y}^i = \\sigma(u^i)$  \n",
    "\n",
    "Chain rule:  \n",
    "$$\\frac{\\partial \\ell}{\\partial \\alpha} = \\sum_{i=1}^m \\frac{\\partial \\ell}{\\partial \\hat{y}^i} \\cdot \\frac{\\partial \\hat{y}^i}{\\partial u^i} \\cdot \\frac{\\partial u^i}{\\partial z_1^i} \\cdot \\frac{\\partial z_1^i}{\\partial v^i} \\cdot \\frac{\\partial v^i}{\\partial \\alpha}$$  \n",
    "\n",
    "Each term:\n",
    "\n",
    "$ \\frac{\\partial \\ell}{\\partial \\hat{y}^i} = -2 (y^i - \\hat{y}^i) $\n",
    "\n",
    "$ \\frac{\\partial \\hat{y}^i}{\\partial u^i} = \\hat{y}^i (1 - \\hat{y}^i) $\n",
    "\n",
    "$ \\frac{\\partial u^i}{\\partial z_1^i} = w_1 $\n",
    "\n",
    "$ \\frac{\\partial z_1^i}{\\partial v^i} = z_1^i (1 - z_1^i) $\n",
    "\n",
    "$ \\frac{\\partial v^i}{\\partial \\alpha} = x^i $ \n",
    "\n",
    "Putting all together:  \n",
    "$$\\frac{\\partial \\ell}{\\partial \\alpha} = \\sum_{i=1}^m \\left[ -2 (y^i - \\hat{y}^i) \\hat{y}^i (1 - \\hat{y}^i) w_1 \\cdot z_1^i (1 - z_1^i) \\cdot x^i \\right]$$  \n",
    "\n",
    "Likewise, for $ \\frac{\\partial \\ell}{\\partial \\beta} $:\n",
    "$$\\frac{\\partial \\ell}{\\partial \\beta} = \\sum_{i=1}^m \\left[ -2 (y^i - \\hat{y}^i) \\hat{y}^i (1 - \\hat{y}^i) w_2 \\cdot z_2^i (1 - z_2^i) \\cdot x^i \\right]$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d17c9a9",
   "metadata": {},
   "source": [
    "# 4. Feature selection and change-point detection  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2765a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(6740)\n",
    "\n",
    "# === Q4.1 Mutual Information ===\n",
    "\n",
    "def mutual_information_table(table):\n",
    "    table = np.array(table)\n",
    "    total = np.sum(table)\n",
    "    px = np.sum(table, axis=0) / total\n",
    "    py = np.sum(table, axis=1) / total\n",
    "    pxy = table / total\n",
    "\n",
    "    mi = 0.0\n",
    "    for i in range(2):  # spam = 1 or 0\n",
    "        for j in range(2):  # word present = 1 or 0\n",
    "            if pxy[i, j] > 0:\n",
    "                mi += pxy[i, j] * np.log2(pxy[i, j] / (py[i] * px[j]))\n",
    "    return mi\n",
    "\n",
    "# Tables for \"prize\" and \"hello\"\n",
    "prize_table = [[130, 15], [1200, 13000]]\n",
    "hello_table = [[160, 25], [13000, 7500]]\n",
    "\n",
    "mi_prize = mutual_information_table(prize_table)\n",
    "mi_hello = mutual_information_table(hello_table)\n",
    "\n",
    "# === Q4.2 CUSUM Change Point Detection ===\n",
    "\n",
    "# Generate 150 samples: first 100 from N(0,1), next 50 from N(0,1.3)\n",
    "x0 = np.random.normal(loc=0, scale=1.0, size=100)\n",
    "x1 = np.random.normal(loc=0, scale=np.sqrt(1.3), size=50)\n",
    "x = np.concatenate([x0, x1])\n",
    "\n",
    "# LLR between f0=N(0,1) and f1=N(0,1.3)\n",
    "def llr(xi):\n",
    "    return -0.5 * np.log(1.3) + 0.5 * xi**2 * (1 - 1/1.3)\n",
    "\n",
    "# Compute CUSUM\n",
    "cusum = [0]\n",
    "for i in range(1, len(x)):\n",
    "    s = max(0, cusum[-1] + llr(x[i]))\n",
    "    cusum.append(s)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(cusum, label=\"CUSUM Statistic\")\n",
    "plt.axvline(100, color='r', linestyle='--', label=\"True Change Point\")\n",
    "plt.title(\"CUSUM Change Point Detection\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"CUSUM Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Mutual Information\", dataframe=pd.DataFrame({\n",
    "    \"Keyword\": [\"prize\", \"hello\"],\n",
    "    \"Mutual Information\": [mi_prize, mi_hello]\n",
    "}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
