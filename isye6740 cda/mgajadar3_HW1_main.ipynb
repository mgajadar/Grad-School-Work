{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6076c72",
   "metadata": {},
   "source": [
    "Question 1   \n",
    "\n",
    "1.1  \n",
    "Supervised learning uses labeled data to learn a mapping from inputs to known outputs. The goal is to train a model that can predict the output for new inputs based on the labeled training data. Common examples include classification and regression.\n",
    "\n",
    "Unsupervised learning deals with unlabeled data. The goal is to uncover patterns, structure, or groupings in the data without being given specific outcomes. Common examples include clustering and dimensionality reduction.\n",
    "\n",
    "Supervised learning advantages:  \n",
    "i. It typically provides higher predictive accuracy when good labeled data is available.  \n",
    "ii. The evaluation of model performance is straightforward using metrics like accuracy or mean squared error.  \n",
    "iii. It can be applied directly to real-world tasks that require prediction or classification.  \n",
    "\n",
    "Supervised learning limitations:  \n",
    "i. Requires a large amount of labeled data, which may be expensive or time-consuming to obtain.  \n",
    "ii. May overfit to the training data, especially when the model is too complex or the dataset is noisy.  \n",
    "\n",
    "Unsupervised learning advantages:  \n",
    "i. Does not require labeled data, which makes it easier to apply in many situations.  \n",
    "ii. Useful for discovering hidden structures or natural groupings in the data.  \n",
    "iii. Can be used for feature extraction or data preprocessing for other tasks.   \n",
    "\n",
    "Unsupervised learning limitations:  \n",
    "i. Difficult to evaluate performance objectively due to lack of ground truth.  \n",
    "ii. The results can be harder to interpret and validate, especially when clusters or components do not align with known categories.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16af8229",
   "metadata": {},
   "source": [
    "1.2  \n",
    "The dataset includes both categorical and numerical features. For example, a data point might look like  \n",
    "$x^1 = \\{\\text{\"Atlanta\"}, \\text{\"house\"}, 500k\\}$    \n",
    "$x^2 = \\{\\text{\"Dallas\"}, \\text{\"house\"}, 300k\\}$  \n",
    "\n",
    "To define a similarity function, it makes sense to handle the two types of features separately. Categorical features can be encoded using one-hot vectors, and their dissimilarity can be measured using Hamming distance, which counts how many feature values differ. For numerical features like price, Euclidean distance is more appropriate.\n",
    "\n",
    "Let  \n",
    "$d_{\\text{cat}}(x_i, x_j)$ be the Hamming distance between the categorical parts    \n",
    "$d_{\\text{num}}(x_i, x_j)$ be the Euclidean distance between the numerical parts  \n",
    "\n",
    "Then we define the combined similarity function as:  \n",
    "$$d(x_i, x_j) = \\alpha \\cdot d_{\\text{cat}}(x_i, x_j) + \\beta \\cdot d_{\\text{num}}(x_i, x_j)$$  \n",
    "\n",
    "Here, $\\alpha$ and $\\beta$ are positive weights that balance the influence of each component\n",
    "\n",
    "This formulation is reasonable because it treats each feature type appropriately and allows for tuning based on the scale or relevance of the features. It also ensures that both types of features contribute meaningfully to the similarity score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f17f59",
   "metadata": {},
   "source": [
    "1.3  \n",
    "We are asked to show that the following two clustering assignment formulations are equivalent:  \n",
    "$$\\pi(i) = \\arg\\min_{j = 1, \\dots, k} \\|x_i - c_j\\|^2\n",
    "\\quad \\text{is equivalent to} \\quad\n",
    "\\pi(i) = \\arg\\min_{j = 1, \\dots, k} c_j^T\\left( \\frac{1}{2}c_j - x_i \\right)$$   \n",
    "\n",
    "To see this, we start by expanding the squared Euclidean distance:  \n",
    "$$\\|x_i - c_j\\|^2 = (x_i - c_j)^T(x_i - c_j)\n",
    "= x_i^T x_i - 2x_i^T c_j + c_j^T c_j$$  \n",
    "\n",
    "Since $x_i^T x_i$ is constant with respect to $j$, it does not affect the argmin. So we can remove it from the expression:  \n",
    "$$\\pi(i) = \\arg\\min_j \\left( -2x_i^T c_j + c_j^T c_j \\right)$$\n",
    "\n",
    "Now factor the expression:  \n",
    "$$-2x_i^T c_j + c_j^T c_j = c_j^T c_j - 2x_i^T c_j\n",
    "= c_j^T\\left(c_j - 2x_i\\right)\n",
    "= 2 \\cdot c_j^T\\left( \\frac{1}{2}c_j - x_i \\right)$$\n",
    "\n",
    "Again, since multiplying by 2 does not change the argmin, we get:  \n",
    "$$\\pi(i) = \\arg\\min_j c_j^T\\left( \\frac{1}{2}c_j - x_i \\right)$$\n",
    "\n",
    "This proves that the two formulations are mathematically equivalent. The second form is often used for vectorized implementations because it can be expressed more easily in matrix notation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c60c06",
   "metadata": {},
   "source": [
    "1.4  \n",
    "K-means is sensitive to how the initial centroids are chosen because the algorithm converges to a local minimum. If the initial centroids are poorly placed, the final clusters might not reflect the true structure in the data. This is especially noticeable in datasets with overlapping or unevenly sized clusters.  \n",
    "\n",
    "Different initializations can lead to different results because each run might converge to a different local minimum of the objective function. Since K-means does not guarantee a global minimum, the clustering result can vary significantly depending on where the centroids start.  \n",
    "\n",
    "To address this, a common strategy is to run the algorithm multiple times with different random initializations and choose the clustering result with the lowest total within-cluster variance. Another approach is to use k-means++, which selects initial centroids in a more informed way by spreading them out in the data space. This tends to produce better results and improves both the speed and stability of convergence  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1285447",
   "metadata": {},
   "source": [
    "1.5  \n",
    "K-means will almost always terminate in a finite number of iterations when using the standard stopping criterion: stop when the cluster assignments no longer change between iterations.  \n",
    "\n",
    "At each step, K-means either maintains or decreases the total within-cluster variance (also called the distortion or objective function). Since the number of possible assignments is finite (though large), and the objective function decreases monotonically or remains constant, the algorithm cannot run forever. It will eventually reach a point where no further updates to cluster assignments are made.  \n",
    "\n",
    "This convergence happens even if the algorithm reaches a local minimum rather than the global one. In practice, this is why it’s common to place a maximum iteration cap as a safety check, but under the usual assumptions and stopping rule, infinite looping doesn’t occur.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4156cdd9",
   "metadata": {},
   "source": [
    "1.6  \n",
    "Adjacency Matrix $A$:  \n",
    "$A =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "1 & 1 & 1 & 0 & 0 \\\\\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "Degree Matrix $D$:  \n",
    "$D =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 3 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Laplacian $L = D - A$:  \n",
    "$L =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & -1 \\\\\n",
    "0 & 1 & 0 & 0 & -1 \\\\\n",
    "0 & 0 & 1 & 0 & -1 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "-1 & -1 & -1 & 0 & 3 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "There are two zero eigenvalues, indicating two disconnected clusters:   \n",
    "Cluster 1: {1, 2, 3, 5}  \n",
    "Cluster 2: {4}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687c261f",
   "metadata": {},
   "source": [
    "Question 2  \n",
    "\n",
    "2.1  \n",
    "We are given the distortion function:  \n",
    "$$J = \\frac{1}{mk} \\sum_{i=1}^m \\sum_{j=1}^k r_{ij} \\|x_i - \\mu_j\\|^2$$ \n",
    "where  \n",
    "$r_{ij} = 1$ if point $x_i$ is assigned to cluster $j$, and 0 otherwise  \n",
    "$\\mu_j$ is the centroid of cluster $j$  \n",
    "\n",
    "To minimize $J$ with respect to $\\mu_j$, we treat the assignments $r_{ij}$ as fixed and take the partial derivative of $J$ with respect to $\\mu_j$:  \n",
    "$$\\frac{\\partial J}{\\partial \\mu_j} = \\frac{\\partial}{\\partial \\mu_j} \\left( \\frac{1}{mk} \\sum_{i=1}^m r_{ij} \\|x_i - \\mu_j\\|^2 \\right)$$  \n",
    "Note that we can focus only on terms involving $\\mu_j$, since other terms vanish:  \n",
    "$$\\frac{\\partial J}{\\partial \\mu_j} = \\frac{1}{mk} \\sum_{i=1}^m r_{ij} \\cdot \\frac{\\partial}{\\partial \\mu_j} \\|x_i - \\mu_j\\|^2$$  \n",
    "Using the identity:  \n",
    "$$\\frac{\\partial}{\\partial \\mu_j} \\|x_i - \\mu_j\\|^2 = \\frac{\\partial}{\\partial \\mu_j} ((x_i - \\mu_j)^T(x_i - \\mu_j)) = -2(x_i - \\mu_j)$$  \n",
    "So:  \n",
    "$$\\frac{\\partial J}{\\partial \\mu_j} = \\frac{1}{mk} \\sum_{i=1}^m r_{ij} \\cdot (-2)(x_i - \\mu_j)$$  \n",
    "$$= -\\frac{2}{mk} \\left( \\sum_{i=1}^m r_{ij} x_i - \\sum_{i=1}^m r_{ij} \\mu_j \\right)$$  \n",
    "$$= -\\frac{2}{mk} \\left( \\sum_{i=1}^m r_{ij} x_i - \\mu_j \\sum_{i=1}^m r_{ij} \\right)$$  \n",
    "Set the derivative equal to zero:  \n",
    "$$\\sum_{i=1}^m r_{ij} x_i = \\mu_j \\sum_{i=1}^m r_{ij}$$  \n",
    "Solving for $\\mu_j$:  \n",
    "$$\\mu_j = \\frac{\\sum_{i=1}^m r_{ij} x_i}{\\sum_{i=1}^m r_{ij}}$$  \n",
    "This shows that the centroid $\\mu_j$ that minimizes the distortion function is simply the mean of the data points assigned to cluster $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc497cd",
   "metadata": {},
   "source": [
    "2.2  \n",
    "We are given the distortion function:  \n",
    "$$J = \\frac{1}{mk} \\sum_{i=1}^m \\sum_{j=1}^k r_{ij} \\|x_i - \\mu_j\\|^2$$  \n",
    "Now, instead of solving for the optimal centroids $\\mu_j$, we fix the centroids and want to find the best assignments $r_{ij}$ that minimize $J$  \n",
    "Since $r_{ij} \\in \\{0, 1\\}$ and for each $i$, $\\sum_{j=1}^k r_{ij} = 1$, this means each data point must belong to exactly one cluster.  \n",
    "To minimize $J$, for each point $x_i$, we want to choose the index $j$ such that $\\|x_i - \\mu_j\\|^2$ is minimized. So we assign each point to its nearest centroid:  \n",
    "$$r_{ij} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } j = \\arg\\min_{l} \\|x_i - \\mu_l\\|^2 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$  \n",
    "This means that for a fixed set of centroids, each data point should be assigned to the cluster whose centroid is closest in terms of squared Euclidean distance.  \n",
    "\n",
    "This step corresponds to the assignment step in the K-means algorithm, and it ensures that the distortion is minimized given the current centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15213d76",
   "metadata": {},
   "source": [
    "2.3\n",
    "Now we are given a modified distortion function that uses the Mahalanobis distance instead of the standard Euclidean distance:  \n",
    "$$J = \\frac{1}{mk} \\sum_{i=1}^m \\sum_{j=1}^k r_{ij} (x_i - \\mu_j)^T \\Sigma (x_i - \\mu_j)$$  \n",
    "where $\\Sigma$ is a fixed, symmetric, positive definite matrix.  \n",
    "We are again interested in minimizing $J$ with respect to $\\mu_j$ and $r_{ij}$  \n",
    "First, minimizing with respect to $\\mu_j$:  \n",
    "Assume assignments $r_{ij}$ are fixed. We take the gradient of $J$ with respect to $\\mu_j$:  \n",
    "$$\\frac{\\partial J}{\\partial \\mu_j}\n",
    "= \\frac{1}{mk} \\sum_{i=1}^m r_{ij} \\cdot \\frac{\\partial}{\\partial \\mu_j} (x_i - \\mu_j)^T \\Sigma (x_i - \\mu_j)$$  \n",
    "Using the identity:  \n",
    "$$\\frac{\\partial}{\\partial \\mu_j} (x_i - \\mu_j)^T \\Sigma (x_i - \\mu_j) = -2 \\Sigma (x_i - \\mu_j)$$  \n",
    "we get:  \n",
    "$$\\frac{\\partial J}{\\partial \\mu_j} = -\\frac{2}{mk} \\sum_{i=1}^m r_{ij} \\Sigma (x_i - \\mu_j)\n",
    "= -\\frac{2}{mk} \\Sigma \\left( \\sum_{i=1}^m r_{ij} x_i - \\mu_j \\sum_{i=1}^m r_{ij} \\right)$$  \n",
    "Setting the gradient to zero:  \n",
    "$$\\Sigma \\left( \\sum_{i=1}^m r_{ij} x_i - \\mu_j \\sum_{i=1}^m r_{ij} \\right) = 0$$  \n",
    "Since $\\Sigma$ is invertible:  \n",
    "$$\\sum_{i=1}^m r_{ij} x_i = \\mu_j \\sum_{i=1}^m r_{ij}\n",
    "\\quad \\Rightarrow \\quad\n",
    "\\mu_j = \\frac{\\sum_{i=1}^m r_{ij} x_i}{\\sum_{i=1}^m r_{ij}}$$  \n",
    "So even with Mahalanobis distance, the centroid update is the same as in standard K-means.  \n",
    "Second, minimizing with respect to $r_{ij}$  \n",
    "Assume $\\mu_j$ and $\\Sigma$ are fixed. For each point $x_i$, we assign it to the cluster $j$ that minimizes the Mahalanobis distance:  \n",
    "$$r_{ij} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } j = \\arg\\min_l (x_i - \\mu_l)^T \\Sigma (x_i - \\mu_l) \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$  \n",
    "This generalizes the assignment step from Euclidean distance to Mahalanobis distance, accounting for correlations or scaling between features via $\\Sigma$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed635d2",
   "metadata": {},
   "source": [
    "Question 3.1 + 3.2: Image Compression using K-means\n",
    "\n",
    "This section applies our custom K-means implementation with squared Euclidean distance/Manhattan distance to three color images: `georgia-aquarium.jpg`, `football.bmp`, and `food.jpg` . For each image, we test k = 5, 10, 20, 30, 40.   The best seed is selected based on image quality (lowest reconstruction error).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9894d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "class KMeansImpl:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load_image(self, image_name=\"data/1.jpeg\", resize_to=(400, 300)):\n",
    "        if not os.path.exists(image_name):\n",
    "            raise FileNotFoundError(f\"Image file not found: {image_name}\")\n",
    "\n",
    "        img = Image.open(image_name)\n",
    "        if resize_to:\n",
    "            img = img.resize(resize_to)\n",
    "        return np.array(img)\n",
    "\n",
    "    def compress(self, pixels, num_clusters, norm_distance=2, seed=None):\n",
    "        map = {\n",
    "            \"class\": None,\n",
    "            \"centroid\": None,\n",
    "            \"img\": None,\n",
    "            \"number_of_iterations\": None,\n",
    "            \"time_taken\": None,\n",
    "            \"additional_args\": {}\n",
    "        }\n",
    "\n",
    "        original_shape = pixels.shape\n",
    "        flat_pixels = pixels.reshape(-1, 3).astype(float)\n",
    "        n_samples = flat_pixels.shape[0]\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        init_indices = np.random.choice(n_samples, num_clusters, replace=False)\n",
    "        centroids = flat_pixels[init_indices]\n",
    "\n",
    "        max_iter = 100\n",
    "        tol = 1e-4\n",
    "        prev_centroids = None\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for iteration in range(max_iter):\n",
    "            if norm_distance == 2:\n",
    "                distances = np.linalg.norm(flat_pixels[:, None, :] - centroids[None, :, :], axis=2) ** 2\n",
    "            elif norm_distance == 1:\n",
    "                distances = np.sum(np.abs(flat_pixels[:, None, :] - centroids[None, :, :]), axis=2)\n",
    "            else:\n",
    "                raise ValueError(\"norm_distance must be 1 or 2\")\n",
    "\n",
    "            assignments = np.argmin(distances, axis=1)\n",
    "\n",
    "            new_centroids = np.array([\n",
    "                flat_pixels[assignments == j].mean(axis=0) if np.any(assignments == j) else centroids[j]\n",
    "                for j in range(num_clusters)\n",
    "            ])\n",
    "\n",
    "            if prev_centroids is not None and np.linalg.norm(new_centroids - prev_centroids) < tol:\n",
    "                break\n",
    "\n",
    "            prev_centroids = centroids\n",
    "            centroids = new_centroids\n",
    "\n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "\n",
    "        compressed_pixels = centroids[assignments].astype(np.uint8)\n",
    "        compressed_img = compressed_pixels.reshape(original_shape)\n",
    "\n",
    "        map[\"class\"] = assignments.reshape(original_shape[0], original_shape[1])\n",
    "        map[\"centroid\"] = centroids\n",
    "        map[\"img\"] = compressed_img\n",
    "        map[\"number_of_iterations\"] = iteration + 1\n",
    "        map[\"time_taken\"] = time_taken\n",
    "\n",
    "        return map\n",
    "\n",
    "    def run_all(self):\n",
    "        k_values = [5, 10, 20, 30, 40]\n",
    "        seed_range = range(3)\n",
    "        norm_distance = 2  # L2\n",
    "        image_paths = {\n",
    "            \"aquarium\": \"data/georgia-aquarium.jpg\",\n",
    "            \"football\": \"data/football.bmp\",\n",
    "            \"custom\": \"data/food.jpg\"\n",
    "        }\n",
    "\n",
    "        for name_tag, image_path in image_paths.items():\n",
    "            print(f\"\\nRunning K-means (L2) on {name_tag}\")\n",
    "            try:\n",
    "                image = self.load_image(image_path, resize_to=(400, 300))\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"  [Skipped] {e}\")\n",
    "                continue\n",
    "\n",
    "            for k in k_values:\n",
    "                best_result = None\n",
    "                min_error = float('inf')\n",
    "\n",
    "                for seed in seed_range:\n",
    "                    result = self.compress(image, k, norm_distance=norm_distance, seed=seed)\n",
    "\n",
    "                    assignments = result[\"class\"].reshape(-1)\n",
    "                    centroids = result[\"centroid\"]\n",
    "                    flat_image = image.reshape(-1, 3).astype(float)\n",
    "                    compressed_flat = centroids[assignments].astype(float)\n",
    "                    error = np.linalg.norm(flat_image - compressed_flat)\n",
    "\n",
    "                    if error < min_error:\n",
    "                        min_error = error\n",
    "                        best_result = result\n",
    "                        best_result[\"seed\"] = seed\n",
    "\n",
    "                out_path = f\"compressed_{name_tag}_k{k}.png\"\n",
    "                Image.fromarray(best_result[\"img\"]).save(out_path)\n",
    "\n",
    "                print(f\"  k = {k}\")\n",
    "                print(f\"    Best Seed: {best_result['seed']}\")\n",
    "                print(f\"    Iterations: {best_result['number_of_iterations']}\")\n",
    "                print(f\"    Time: {best_result['time_taken']:.2f} seconds\")\n",
    "        \n",
    "    def run_all_l1(self):\n",
    "        k_values = [5, 10, 20, 30, 40]\n",
    "        seed_range = range(3)\n",
    "        norm_distance = 1  # Manhattan distance\n",
    "        image_paths = {\n",
    "            \"aquarium\": \"data/georgia-aquarium.jpg\",\n",
    "            \"football\": \"data/football.bmp\",\n",
    "            \"custom\": \"data/food.jpg\"\n",
    "        }\n",
    "\n",
    "        for name_tag, image_path in image_paths.items():\n",
    "            print(f\"\\nRunning K-means (L1) on {name_tag}\")\n",
    "            try:\n",
    "                image = self.load_image(image_path, resize_to=(400, 300))\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"  [Skipped] {e}\")\n",
    "                continue\n",
    "\n",
    "            for k in k_values:\n",
    "                best_result = None\n",
    "                min_error = float('inf')\n",
    "\n",
    "                for seed in seed_range:\n",
    "                    result = self.compress(image, k, norm_distance=norm_distance, seed=seed)\n",
    "\n",
    "                    assignments = result[\"class\"].reshape(-1)\n",
    "                    centroids = result[\"centroid\"]\n",
    "                    flat_image = image.reshape(-1, 3).astype(float)\n",
    "                    compressed_flat = centroids[assignments].astype(float)\n",
    "                    error = np.linalg.norm(flat_image - compressed_flat)\n",
    "\n",
    "                    if error < min_error:\n",
    "                        min_error = error\n",
    "                        best_result = result\n",
    "                        best_result[\"seed\"] = seed\n",
    "\n",
    "                out_path = f\"compressed_L1_{name_tag}_k{k}.png\"\n",
    "                Image.fromarray(best_result[\"img\"]).save(out_path)\n",
    "\n",
    "                print(f\"  k = {k}\")\n",
    "                print(f\"    Best Seed: {best_result['seed']}\")\n",
    "                print(f\"    Iterations: {best_result['number_of_iterations']}\")\n",
    "                print(f\"    Time: {best_result['time_taken']:.2f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    kmeans = KMeansImpl()\n",
    "    kmeans.run_all()\n",
    "    kmeans.run_all_l1()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93bc851",
   "metadata": {},
   "source": [
    "3.1: K-means with Squared Euclidean (L2) Distance  \n",
    "\n",
    "We applied K-means clustering with squared Euclidean distance to compress three RGB images: georgia-aquarium.jpg, football.bmp, and a custom image (food.jpg), all resized to 400x300. For each image, we tested k-values of 5, 10, 20, 30, and 40. For each k, we ran K-means three times using different random seeds (0, 1, 2) and selected the result with the lowest reconstruction error.\n",
    "\n",
    "Georgia Aquarium (L2)  \n",
    "| k  | Best Seed | Iterations | Time (s) | Reconstructed Image |  \n",
    "|----|-----------|------------|----------|----------------------|  \n",
    "| 5  | 0         | 58         | 1.82     | ![k=5](compressed_aquarium_k5.png)                       |\n",
    "| 10 | 2         | 76         | 4.38     | ![k=10](compressed_aquarium_k10.png)                     |  \n",
    "| 20 | 0         | 100        | 12.24    | ![k=20](compressed_aquarium_k20.png)                     |  \n",
    "| 30 | 1         | 100        | 15.75    | ![k=30](compressed_aquarium_k30.png)                     |  \n",
    "| 40 | 1         | 100        | 19.87    | ![k=40](compressed_aquarium_k40.png)                     |  \n",
    "\n",
    "Football (L2)  \n",
    "| k  | Best Seed | Iterations | Time (s) | Reconstructed Image |  \n",
    "|----|-----------|------------|----------|----------------------|  \n",
    "| 5  | 1         | 47         | 1.27     | ![k=5](compressed_football_k5.png)                       |  \n",
    "| 10 | 0         | 100        | 5.35     | ![k=10](compressed_football_k10.png)                     |  \n",
    "| 20 | 1         | 100        | 10.05    | ![k=20](compressed_football_k20.png)                     |  \n",
    "| 30 | 1         | 100        | 15.07    | ![k=30](compressed_football_k30.png)                     |  \n",
    "| 40 | 1         | 100        | 19.35    | ![k=40](compressed_football_k40.png)                     |  \n",
    "\n",
    "Custom Image (L2)  \n",
    "| k  | Best Seed | Iterations | Time (s) | Reconstructed Image |  \n",
    "|----|-----------|------------|----------|----------------------|  \n",
    "| 5  | 2         | 44         | 1.22     | ![k=5](compressed_custom_k5.png)                       |  \n",
    "| 10 | 1         | 85         | 4.30     | ![k=10](compressed_custom_k10.png)                     |  \n",
    "| 20 | 0         | 100        | 9.94     | ![k=20](compressed_custom_k20.png)                     |  \n",
    "| 30 | 2         | 100        | 14.52    | ![k=30](compressed_custom_k30.png)                     |  \n",
    "| 40 | 2         | 100        | 19.21    | ![k=40](compressed_custom_k40.png)                     |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4c794f",
   "metadata": {},
   "source": [
    "3.2: K-means with Manhattan (L1) Distance\n",
    "\n",
    "We repeated the same compression process using Manhattan (L1) distance. The structure of the runs remained identical: same images, k-values, and seed trials.  \n",
    "\n",
    "Georgia Aquarium (L1)  \n",
    "| k  | Best Seed | Iterations | Time (s) | Reconstructed Image |\n",
    "|----|-----------|------------|----------|----------------------|\n",
    "| 5  | 1         | 17         | 0.47     | ![L1 Aquarium k=5](compressed_L1_aquarium_k5.png) |\n",
    "| 10 | 2         | 41         | 2.11     | ![L1 Aquarium k=10](compressed_L1_aquarium_k10.png) |\n",
    "| 20 | 0         | 46         | 4.30     | ![L1 Aquarium k=20](compressed_L1_aquarium_k20.png) |\n",
    "| 30 | 1         | 100        | 13.65    | ![L1 Aquarium k=30](compressed_L1_aquarium_k30.png) |\n",
    "| 40 | 0         | 100        | 17.78    | ![L1 Aquarium k=40](compressed_L1_aquarium_k40.png) |\n",
    "\n",
    "Football (L1)\n",
    "| k  | Best Seed | Iterations | Time (s) | Reconstructed Image |\n",
    "|----|-----------|------------|----------|----------------------|\n",
    "| 5  | 2         | 22         | 0.56     | ![L1 Football k=5](compressed_L1_football_k5.png) |\n",
    "| 10 | 0         | 74         | 3.46     | ![L1 Football k=10](compressed_L1_football_k10.png) |\n",
    "| 20 | 1         | 61         | 5.41     | ![L1 Football k=20](compressed_L1_football_k20.png) |\n",
    "| 30 | 1         | 80         | 10.27    | ![L1 Football k=30](compressed_L1_football_k30.png) |\n",
    "| 40 | 1         | 100        | 19.10    | ![L1 Football k=40](compressed_L1_football_k40.png) |\n",
    "\n",
    "Custom Image (L1)\n",
    "| k  | Best Seed | Iterations | Time (s) | Reconstructed Image |\n",
    "|----|-----------|------------|----------|----------------------|\n",
    "| 5  | 1         | 34         | 0.88     | ![L1 Custom k=5](compressed_L1_custom_k5.png) |\n",
    "| 10 | 1         | 66         | 3.11     | ![L1 Custom k=10](compressed_L1_custom_k10.png) |\n",
    "| 20 | 0         | 100        | 10.28    | ![L1 Custom k=20](compressed_L1_custom_k20.png) |\n",
    "| 30 | 1         | 76         | 9.96     | ![L1 Custom k=30](compressed_L1_custom_k30.png) |\n",
    "| 40 | 0         | 96         | 18.30    | ![L1 Custom k=40](compressed_L1_custom_k40.png) |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be0e7e",
   "metadata": {},
   "source": [
    "Comparison and Interpretation (L1 vs L2)\n",
    "\n",
    "When comparing the K-means clustering results between L1 (Manhattan) and L2 (Euclidean) distances, we observed some consistent trends across all three images. L2 typically produced slightly smoother and more visually continuous results, especially at higher values of k. This is likely due to the way squared error penalizes larger deviations more heavily, which tends to pull centroids toward denser pixel clusters.\n",
    "\n",
    "In contrast, L1 often resulted in more distinct, blocky regions with sharper boundaries between color segments. While the visual quality was still acceptable, it occasionally introduced harsh transitions between similar tones.\n",
    "\n",
    "Performance-wise, L1 generally converged in fewer iterations than L2 at lower k-values, and the runtime per image was slightly faster on average. However, for k = 30 and 40, both methods tended to hit the iteration cap of 100 in several cases.\n",
    "\n",
    "Overall, L2 may be preferred when color smoothness and fidelity are important, while L1 offers competitive results with faster convergence in many scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e7e88c",
   "metadata": {},
   "source": [
    "3.3  \n",
    "To pick the best k, one common method is the elbow method. You look at how the reconstruction error drops as k increases and try to spot the point where it starts to level off. That usually means you’re getting most of the improvement without needing way more clusters.\n",
    "\n",
    "In our case, we didn’t plot that formally, but just based on how the images looked and how long they took to run, k = 20 seemed like a solid middle ground. The quality looked much better than k = 5 or 10, and going up to k = 30 or 40 didn’t always look noticeably better, but took longer and hit the iteration cap more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6d0127",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Question 4\n",
    "# 4.1\n",
    "\n",
    "import numpy as np\n",
    "from os.path import abspath, exists\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.sparse import lil_matrix, diags\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class PoliticalBlogsClustering:\n",
    "    def __init__(self, edge_file='data/edges.txt', node_file='data/nodes.txt'):\n",
    "        self.edge_file = edge_file\n",
    "        self.node_file = node_file\n",
    "\n",
    "    def load_data(self):\n",
    "        self.labels = {}\n",
    "        with open(self.node_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) >= 3:\n",
    "                    self.labels[parts[0]] = int(parts[2])\n",
    "\n",
    "        self.node_list = list(self.labels.keys())\n",
    "        self.node_index = {node_id: i for i, node_id in enumerate(self.node_list)}\n",
    "        n = len(self.node_list)\n",
    "        A = lil_matrix((n, n))\n",
    "\n",
    "        with open(self.edge_file, 'r') as f:\n",
    "            for line in f:\n",
    "                src, dst = line.strip().split()\n",
    "                if src in self.node_index and dst in self.node_index:\n",
    "                    i, j = self.node_index[src], self.node_index[dst]\n",
    "                    A[i, j] = 1\n",
    "                    A[j, i] = 1\n",
    "\n",
    "        self.adj_matrix = A.tocsr()\n",
    "\n",
    "    def compute_laplacian(self):\n",
    "        degrees = np.array(self.adj_matrix.sum(axis=1)).flatten()\n",
    "        D = diags(degrees)\n",
    "        L = D - self.adj_matrix\n",
    "        return L\n",
    "\n",
    "    def find_majority_labels(self, num_clusters=2):\n",
    "        result_map = {\n",
    "            \"overall_mismatch_rate\": None,\n",
    "            \"mismatch_rates\": []\n",
    "        }\n",
    "\n",
    "        self.load_data()\n",
    "        L = self.compute_laplacian()\n",
    "        _, eigvecs = eigsh(L, k=num_clusters, which='SM')\n",
    "        row_norms = np.linalg.norm(eigvecs, axis=1, keepdims=True)\n",
    "        row_norms[row_norms == 0] = 1\n",
    "        norm_rows = eigvecs / row_norms\n",
    "\n",
    "        kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=0)\n",
    "        clusters = kmeans.fit_predict(norm_rows)\n",
    "\n",
    "        cluster_groups = defaultdict(list)\n",
    "        for i, cluster_id in enumerate(clusters):\n",
    "            node_id = self.node_list[i]\n",
    "            label = self.labels[node_id]\n",
    "            cluster_groups[cluster_id].append(label)\n",
    "\n",
    "        total_mismatches = 0\n",
    "        total_nodes = 0\n",
    "\n",
    "        for cluster_id, labels in cluster_groups.items():\n",
    "            count = Counter(labels)\n",
    "            majority_label = count.most_common(1)[0][0]\n",
    "            mismatches = sum(1 for l in labels if l != majority_label)\n",
    "            mismatch_rate = round(mismatches / len(labels), 2)\n",
    "\n",
    "            result_map[\"mismatch_rates\"].append({\n",
    "                \"majority_index\": int(majority_label),\n",
    "                \"mismatch_rate\": mismatch_rate\n",
    "            })\n",
    "\n",
    "            total_mismatches += mismatches\n",
    "            total_nodes += len(labels)\n",
    "\n",
    "        result_map[\"overall_mismatch_rate\"] = round(total_mismatches / total_nodes, 2)\n",
    "        return result_map\n",
    "\n",
    "model = PoliticalBlogsClustering(edge_file='data/edges.txt', node_file='data/nodes.txt')\n",
    "\n",
    "for k in [2, 5, 10, 30, 50]:\n",
    "    print(f\"\\n===== Spectral Clustering with k = {k} =====\")\n",
    "    result = model.find_majority_labels(num_clusters=k)\n",
    "    print(f\"Overall mismatch rate: {result['overall_mismatch_rate']}\")\n",
    "    for i, info in enumerate(result[\"mismatch_rates\"]):\n",
    "        print(f\"  Cluster {i}: majority = {info['majority_index']}, mismatch rate = {info['mismatch_rate']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cad5acf",
   "metadata": {},
   "source": [
    "4.1\n",
    "\n",
    "We applied unnormalized spectral clustering to the political blogs network using k values of 2, 5, 10, 30, and 50. The graph was treated as undirected, and the Laplacian matrix was constructed using the standard definition $L=D-A$. We computed the first k eigenvectors of L, normalized the rows, and used K-means to assign nodes into clusters.\n",
    "\n",
    "For each cluster, we identified the majority political label (0 or 1) and calculated the mismatch rate, which is the fraction of nodes in that cluster that did not match the majority label. We then averaged mismatches across all clusters to compute an overall mismatch rate.\n",
    "\n",
    "The results are summarized below:   \n",
    "| k   | Overall Mismatch Rate |  \n",
    "|-----|------------------------|  \n",
    "| 2   | 0.47                   |  \n",
    "| 5   | 0.46                   |  \n",
    "| 10  | 0.46                   |  \n",
    "| 30  | 0.45                   |  \n",
    "| 50  | 0.45                   |  \n",
    "\n",
    "We noticed that increasing k slightly reduced the mismatch rate, but the difference between k = 10 and k = 50 was minimal. Also, several clusters at higher k values had high internal mismatch rates, indicating that additional clusters did not always yield more consistent groupings.\n",
    "\n",
    "4.2  \n",
    "\n",
    "To tune k, we evaluated the overall mismatch rate for multiple values: k = 2, 5, 10, 30, and 50. Our goal was to find a value of k that minimized the mismatch rate while still producing meaningful groupings. We used the mismatch rate as a measure of how politically consistent each cluster was, where lower mismatch indicates that a cluster mostly contains blogs with the same political leaning.\n",
    "\n",
    "As k increased, the mismatch rate dropped slightly, from 0.47 at k = 2 to 0.45 at k = 50. However, most of that improvement had already occurred by k = 10. Beyond that, the mismatch rate barely changed, and many individual clusters still had high internal disagreement.\n",
    "\n",
    "Based on this trend, we selected k = 10, which achieved an overall mismatch rate of 0.46. This was nearly as low as k = 50, but with fewer, more interpretable clusters and lower computational cost.\n",
    "\n",
    "Intuitively, this result suggests that the political blog network has some community structure, but it isn’t strongly polarized into perfectly consistent groups. While clusters tend to lean toward one label, many still contain a mix of both. This may reflect overlapping topics, link sharing across ideological lines, or limitations of using only link structure to infer political identity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a3d775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'true_label': 1, 'purity_score': 0.5327}, {'true_label': 4, 'purity_score': 0.3572}, {'true_label': 0, 'purity_score': 0.7938}, {'true_label': 3, 'purity_score': 0.526}, {'true_label': 1, 'purity_score': 0.6231}, {'true_label': 8, 'purity_score': 0.5261}, {'true_label': 7, 'purity_score': 0.4266}, {'true_label': 2, 'purity_score': 0.8951}, {'true_label': 0, 'purity_score': 0.9048}, {'true_label': 6, 'purity_score': 0.8593}]\n",
      "[{'true_label': 1, 'purity_score': 0.1124}]\n"
     ]
    }
   ],
   "source": [
    "# Question 5\n",
    "# 5.1\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.io import loadmat\n",
    "\n",
    "class MNISTClustering:\n",
    "    def __init__(self, data_file='data/mnist_10digits.mat', n_clusters=10):\n",
    "        self.data_file = data_file\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "    def purity_scores(self, norm_distance=2, binary=False):\n",
    "        \"\"\"\n",
    "        Perform clustering on MNIST data and compute purity score.\n",
    "\n",
    "        Args:\n",
    "            norm_distance (int): 2 for Euclidean, 1 for Hamming\n",
    "            binary (bool): Whether to binarize data for Hamming clustering\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries with each cluster's majority label and purity score.\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        mat = loadmat(self.data_file)\n",
    "        X = mat['xtrain'].astype(np.float32)\n",
    "        y = mat['ytrain'].flatten()\n",
    "\n",
    "        if binary:\n",
    "            X = (X >= 128).astype(np.uint8)\n",
    "        else:\n",
    "            X = X / 255.0\n",
    "\n",
    "        if norm_distance == 2:\n",
    "            km = KMeans(n_clusters=self.n_clusters, n_init=10, random_state=0)\n",
    "            preds = km.fit_predict(X)\n",
    "\n",
    "        elif norm_distance == 1:\n",
    "            if not binary:\n",
    "                X = (X >= 128).astype(np.uint8)\n",
    "            rng = np.random.default_rng(seed=0)\n",
    "            centroids = X[rng.choice(X.shape[0], self.n_clusters, replace=False)]\n",
    "\n",
    "            for _ in range(10):\n",
    "                dists = np.array([[np.sum(x != c) for c in centroids] for x in X])\n",
    "                preds = np.argmin(dists, axis=1)\n",
    "\n",
    "                for k in range(self.n_clusters):\n",
    "                    members = X[preds == k]\n",
    "                    if len(members) > 0:\n",
    "                        centroids[k] = (np.mean(members, axis=0) > 0.5).astype(np.uint8)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid distance metric.\")\n",
    "\n",
    "        # Purity calculation\n",
    "        results = []\n",
    "        for k in range(self.n_clusters):\n",
    "            indices = np.where(preds == k)[0]\n",
    "            if len(indices) == 0:\n",
    "                continue\n",
    "            cluster_labels = y[indices]\n",
    "            majority_label, majority_count = Counter(cluster_labels).most_common(1)[0]\n",
    "            purity_k = majority_count / len(indices)\n",
    "            results.append({\n",
    "                \"true_label\": int(majority_label),\n",
    "                \"purity_score\": round(purity_k, 4)\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "model = MNISTClustering()\n",
    "print(model.purity_scores(norm_distance=2))\n",
    "print(model.purity_scores(norm_distance=1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0bfbe1",
   "metadata": {},
   "source": [
    "K-Means Clustering Using Euclidean Distance\n",
    "\n",
    "We clustered the MNIST dataset into 10 groups using K-means with Euclidean distance. The raw grayscale image vectors were normalized to the range [0, 1] before clustering. For each cluster, we computed purity by assigning it the most frequent digit label and calculating the fraction of images in the cluster that matched that label.\n",
    "\n",
    "The resulting overall purity was 0.5907. Several clusters had high purity, including:  \n",
    "Cluster 8: 0.9048  \n",
    "Cluster 9: 0.8593  \n",
    "Cluster 7: 0.8951  \n",
    "\n",
    "These results indicate that Euclidean distance was able to capture meaningful structure in the data and grouped similar digits together reasonably well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097a8cc2",
   "metadata": {},
   "source": [
    "5.2 Comparison Between Euclidean and Hamming Clustering\n",
    "\n",
    "After applying both Euclidean-based K-means on grayscale-normalized images and Hamming-based K-means on binarized images, we found that Euclidean distance produced better clustering performance.\n",
    "\n",
    "The overall purity score using Euclidean distance was 0.5907, compared to 0.4961 using Hamming distance. Several clusters in the Euclidean method had purities above 0.85, indicating strong alignment between the clusters and true digit labels. In contrast, the Hamming-based approach produced more evenly distributed clusters in terms of size, but with generally lower purity.\n",
    "\n",
    "This suggests that binarizing the images loses useful pixel intensity information, which is important for distinguishing between similar digits. Euclidean distance benefits from the full grayscale detail, enabling more nuanced separation of digit structures.\n",
    "\n",
    "Conclusion: Clustering using Euclidean distance on normalized grayscale images resulted in better purity scores and more meaningful digit groupings. Therefore, the Euclidean-based method is more effective for this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
